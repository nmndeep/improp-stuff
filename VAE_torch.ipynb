{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "# import tensorflow\n",
    "from  torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128     # number of data points in each batch\n",
    "N_EPOCHS = 10       # times to run the model on complete data\n",
    "INPUT_DIM = 28 * 28 # size of each input\n",
    "HIDDEN_DIM = 256    # hidden dimension\n",
    "LATENT_DIM = 20     # latent vector dimension\n",
    "lr = 1e-3           # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1152):\n",
    "        return input.view(input.size(0), int(size/36), 6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, h_dim=1568, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(8, 16, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, 1),\n",
    "            nn.MaxPool2d(3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc4 = nn.Linear(h_dim, 1152)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(32, 64, 3, stride=1, padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=1, padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn(*mu.size())\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.fc4(z)\n",
    "        z = self.decoder(z)\n",
    "        print(z.shape)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_channels = fixed_x.size(1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "model = VAE().to(device)\n",
    "# model.load_state_dict(torch.load('vae.torch', map_location='cpu'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 390.737488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naman/miniconda3/lib/python3.5/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [128/60000 (0%)]\tLoss: 393.855865\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [256/60000 (0%)]\tLoss: 388.272736\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [384/60000 (1%)]\tLoss: 355.460297\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [512/60000 (1%)]\tLoss: 290.943787\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 190.628937\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [768/60000 (1%)]\tLoss: 5.029020\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [896/60000 (1%)]\tLoss: -242.161102\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1024/60000 (2%)]\tLoss: -789.874878\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1152/60000 (2%)]\tLoss: -1523.656128\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: -3075.064209\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1408/60000 (2%)]\tLoss: -4652.851074\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1536/60000 (3%)]\tLoss: -5882.634277\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1664/60000 (3%)]\tLoss: -6566.900879\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1792/60000 (3%)]\tLoss: -6933.490723\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: -6260.722656\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2048/60000 (3%)]\tLoss: -5572.010742\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2176/60000 (4%)]\tLoss: -5096.684570\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2304/60000 (4%)]\tLoss: -4828.960938\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2432/60000 (4%)]\tLoss: -4989.388672\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: -4761.830078\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2688/60000 (4%)]\tLoss: -4574.679199\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2816/60000 (5%)]\tLoss: -4637.410645\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [2944/60000 (5%)]\tLoss: -4610.454590\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3072/60000 (5%)]\tLoss: -4747.615723\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: -4809.742188\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3328/60000 (6%)]\tLoss: -4915.013184\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3456/60000 (6%)]\tLoss: -5162.286133\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3584/60000 (6%)]\tLoss: -5010.961914\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3712/60000 (6%)]\tLoss: -4757.313965\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: -5022.506348\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [3968/60000 (7%)]\tLoss: -5223.731445\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [4096/60000 (7%)]\tLoss: -5080.615234\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [4224/60000 (7%)]\tLoss: -5453.091309\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Train Epoch: 1 [4352/60000 (7%)]\tLoss: -5323.211426\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3cfd79e6ef55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         to_print = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    for idx, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        recon_images, mu, logvar = model(images)\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        to_print = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch+1, idx * len(images), len(train_loader.dataset),\n",
    "                100. * idx / len(train_loader), loss.data/BATCH_SIZE)\n",
    "#         to                        epochs, loss.data/BATCH_SIZE, bce.data/BATCH_SIZE, kld.data/BATCH_SIZE)\n",
    "        print(to_print)\n",
    "\n",
    "# notify to android when finished training\n",
    "notify(to_print, priority=1)\n",
    "\n",
    "torch.save(vae.state_dict(), 'vae.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare(x):\n",
    "    recon_x, _, _ = vae(x)\n",
    "    return torch.cat([x, recon_x])\n",
    "\n",
    "fixed_x = dataset[randint(1, 100)][0].unsqueeze(0)\n",
    "compare_x = compare(fixed_x)\n",
    "\n",
    "save_image(compare_x.data.cpu(), 'sample_image.png')\n",
    "display(Image('sample_image.png', width=700, unconfined=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
